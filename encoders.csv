encoder,size,desc
all-MiniLM-L6-v2,384,"Compact distilled model with RoBERTa-Large as teacher, version with 6 layers (30M parameters). To obtain the sentence transformer, the model is fine-tuned on a set of 1B sentence pairs with the task: given a sentence from a pair, the model must predict which of a set of other randomly sampled sentences is matched with it."
all-mpnet-base-v2,768,TO COMPLETE
all-distilroberta-v1,768,TO COMPLETE
gtr-t5-large,768,TO COMPLETE
all-roberta-large-v1,1024,TO COMPLETE
